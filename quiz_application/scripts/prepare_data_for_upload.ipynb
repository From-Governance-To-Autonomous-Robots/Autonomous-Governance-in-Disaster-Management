{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_625738/3262155539.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy \n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore, storage\n",
    "import os\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Firebase Admin SDK\n",
    "cred = credentials.Certificate('disaster-master.json')  # Replace with your Firebase Admin SDK key\n",
    "firebase_admin.initialize_app(cred, {\n",
    "    'storageBucket': 'disaster-master-59d6f.appspot.com'  # Replace with your storage bucket URL\n",
    "})\n",
    "\n",
    "db = firestore.client()\n",
    "bucket = storage.bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/question_data_bank.csv\"\n",
    "main_log_dir = \"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/data_logs\"\n",
    "# \"<image> <text> <gt> <pred> <pred_conf> <task> <question_options> <question_format> <question_id> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload image to Firebase Storage\n",
    "def upload_image(image_path):\n",
    "    blob = bucket.blob(os.path.basename(image_path))\n",
    "    blob.upload_from_filename(image_path)\n",
    "    blob.make_public()\n",
    "    return blob.public_url\n",
    "\n",
    "# Function to upload question to Firestore\n",
    "def upload_question(question_data, image_path):\n",
    "    image_url = upload_image(image_path)\n",
    "    question_data['image']= image_url\n",
    "    db.collection('questions').document(f\"q_{question_data['question_id']}\").set(question_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_tsv_data(tsv_files:list,dataset_dir:str):\n",
    "    file_paths = [os.path.join(dataset_dir, file) for file in tsv_files]\n",
    "    dfs = [pd.read_csv(file_path, sep='\\t') for file_path in file_paths]\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    cleaned_df = combined_df[combined_df['image_damage'] != 'dont_know_or_cant_judge']\n",
    "    \n",
    "    cleaned_df.to_csv(os.path.join(dataset_dir,'whole_dataset.csv'), index=False)\n",
    "    \n",
    "    return cleaned_df,os.path.join(dataset_dir,'whole_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save_dataset(dataset, target_column, test_size, random_state, output_dir,task):\n",
    "    train_data, val_data = train_test_split(\n",
    "        dataset, test_size=test_size, random_state=random_state, stratify=dataset[target_column]\n",
    "    )\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    train_path = os.path.join(output_dir, f'train_{task}_dataset.csv')\n",
    "    val_path = os.path.join(output_dir, f'val_{task}_dataset.csv')\n",
    "    \n",
    "    train_data.to_csv(train_path, index=False)\n",
    "    val_data.to_csv(val_path, index=False)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path='config.yaml'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_n_records(df, n,target_class,task):\n",
    "    \n",
    "    informative_df = df[df[f'image_{task}'] == target_class]\n",
    "    \n",
    "    if len(informative_df) < n:\n",
    "        print(f\"Warning: Only {len(informative_df)} informative records available. Returning all of them.\")\n",
    "        return informative_df\n",
    "    \n",
    "    return informative_df.sample(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_n_records_post_disaster(df, n, target_class):\n",
    "    if n % 2 != 0:\n",
    "        n+=1\n",
    "        # raise ValueError(\"n must be an even number to ensure a 50:50 ratio.\")\n",
    "\n",
    "    half_n = n // 2\n",
    "\n",
    "    # Filter the DataFrame for records where target_class is 0 and 1\n",
    "    class_0_df = df[df[target_class] == 0]\n",
    "    class_1_df = df[df[target_class] == 1]\n",
    "\n",
    "    # Check if there are enough records in each class\n",
    "    if len(class_0_df) < half_n :\n",
    "        sample_class_0 = class_0_df\n",
    "        # raise ValueError(f\"Not e?nough records to select {half_n} samples from each class.\")\n",
    "    else:\n",
    "        sample_class_0 = class_0_df.sample(n=half_n)\n",
    "    \n",
    "    if len(class_1_df) < half_n:\n",
    "        sample_class_1 = class_1_df\n",
    "    else:\n",
    "        sample_class_1 = class_1_df.sample(n=half_n)\n",
    "\n",
    "    # Combine the samples and shuffle the resulting DataFrame\n",
    "    combined_df = pd.concat([sample_class_0, sample_class_1]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_config = load_config(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/disaster_classification/configs/info_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, whole_dataset_csv_path = load_and_combine_tsv_data(info_config['paths']['tsv_files'], dataset_dir=info_config['paths']['dataset_dir_path'])\n",
    "\n",
    "# # Create folders\n",
    "# log_dir = os.path.join(main_log_dir,'multimodal_logs',info_config['paths']['task'])\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # Filter to use only the dataset for the task columns \n",
    "# text_target_column = f\"image_{info_config['paths']['task']}\"\n",
    "# filtered_dataset = dataset[['tweet_text', text_target_column, 'image_path']]\n",
    "# filtered_dataset = filtered_dataset.dropna(subset=[text_target_column, 'image_path'])\n",
    "\n",
    "# # Apply class relabeling if specified in the config\n",
    "# if 'classes_to_relabel' in info_config['data'] and info_config['data']['classes_to_relabel']:\n",
    "#     classes_to_relabel = info_config['data']['classes_to_relabel'] \n",
    "#     filtered_dataset[text_target_column] = filtered_dataset[text_target_column].replace(classes_to_relabel)\n",
    "\n",
    "# # Task specific operations - drop rows with certain classes to reduce noise \n",
    "# if len(info_config['data']['classes_to_drop']) > 0:\n",
    "#     for class_drop in info_config['data']['classes_to_drop']:\n",
    "#         filtered_dataset = filtered_dataset[filtered_dataset[text_target_column] != class_drop]\n",
    "        \n",
    "# train_data, val_data = split_and_save_dataset(filtered_dataset, text_target_column, info_config['model_training_parameters']['test_size'], info_config['model_training_parameters']['random_state'], log_dir, text_target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/data_logs/multimodal_logs/info/train_image_info_dataset.csv\")\n",
    "val_data = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/data_logs/multimodal_logs/info/val_image_info_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image_info</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hurricane Harvey Relief Comes With an Extra-La...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>data_image/hurricane_harvey/14_9_2017/90818048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mexico City Sport Director Says No Major Damag...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>data_image/mexico_earthquake/20_9_2017/9105413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hurricane Maria still has strength. Its impact...</td>\n",
       "      <td>informative</td>\n",
       "      <td>data_image/hurricane_maria/23_9_2017/911530504...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @johnrobertsFox: Looking to fly out of Hurr...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>data_image/hurricane_irma/7_9_2017/90562524655...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UPDATE: Iran-Iraq earthquake • Death toll at 4...</td>\n",
       "      <td>informative</td>\n",
       "      <td>data_image/iraq_iran_earthquake/13_11_2017/930...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text       image_info  \\\n",
       "0  Hurricane Harvey Relief Comes With an Extra-La...  not_informative   \n",
       "1  Mexico City Sport Director Says No Major Damag...  not_informative   \n",
       "2  Hurricane Maria still has strength. Its impact...      informative   \n",
       "3  RT @johnrobertsFox: Looking to fly out of Hurr...  not_informative   \n",
       "4  UPDATE: Iran-Iraq earthquake • Death toll at 4...      informative   \n",
       "\n",
       "                                          image_path  \n",
       "0  data_image/hurricane_harvey/14_9_2017/90818048...  \n",
       "1  data_image/mexico_earthquake/20_9_2017/9105413...  \n",
       "2  data_image/hurricane_maria/23_9_2017/911530504...  \n",
       "3  data_image/hurricane_irma/7_9_2017/90562524655...  \n",
       "4  data_image/iraq_iran_earthquake/13_11_2017/930...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_training_samples = select_random_n_records(train_data,25,'informative',\"info\")\n",
    "noninfo_training_samples = select_random_n_records(train_data,25,'not_informative',\"info\")\n",
    "\n",
    "training_samples_df = pd.concat([info_training_samples,noninfo_training_samples],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_validation_samples = select_random_n_records(val_data,50,'informative',\"info\")\n",
    "noninfo_validation_samples = select_random_n_records(val_data,50,'not_informative',\"info\")\n",
    "\n",
    "validation_samples_df = pd.concat([info_validation_samples,noninfo_validation_samples],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    \"informative\":0,\n",
    "    \"not_informative\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading for training: 50it [00:10,  4.66it/s]\n",
      "Uploading for validation: 100it [00:18,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "question_record = {\n",
    "    'question_id': 0,\n",
    "    'image': None,\n",
    "    'text': None,\n",
    "    'pred':None,\n",
    "    'pred_conf':0,\n",
    "    'task': 'info',\n",
    "    'phase':None,\n",
    "    'correct_answer': None,\n",
    "    'question_options': ['Informative','Not-Informative', 'Gather Additional Data'],\n",
    "    'question_format': 'Does the Image and Text data contain information relevant to the Disaster ?'\n",
    "}\n",
    "\n",
    "for index,row in tqdm(training_samples_df.iterrows(),desc=\"Uploading for training\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'train'\n",
    "    image_path = os.path.join(info_config['paths']['dataset_dir_path'],row['image_path'])\n",
    "    question_record['text'] = row['tweet_text']\n",
    "    question_record['correct_answer'] = class_names[row['image_info']]\n",
    "    upload_question(question_record,image_path)\n",
    "\n",
    "\n",
    "for index,row in tqdm(validation_samples_df.iterrows(),desc=\"Uploading for validation\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'val'\n",
    "    image_path = os.path.join(info_config['paths']['dataset_dir_path'],row['image_path'])\n",
    "    question_record['text'] = row['tweet_text']\n",
    "    question_record['correct_answer'] = class_names[row['image_info']]\n",
    "    upload_question(question_record,image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humanitarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_config = load_config(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/disaster_classification/configs/human_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, whole_dataset_csv_path = load_and_combine_tsv_data(human_config['paths']['tsv_files'], dataset_dir=human_config['paths']['dataset_dir_path'])\n",
    "\n",
    "# # Create folders\n",
    "# log_dir = os.path.join(main_log_dir,'multimodal_logs',human_config['paths']['task'])\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # Filter to use only the dataset for the task columns \n",
    "# text_target_column = f\"image_{human_config['paths']['task']}\"\n",
    "# filtered_dataset = dataset[['tweet_text', text_target_column, 'image_path']]\n",
    "# filtered_dataset = filtered_dataset.dropna(subset=[text_target_column, 'image_path'])\n",
    "\n",
    "# # Apply class relabeling if specified in the config\n",
    "# if 'classes_to_relabel' in human_config['data'] and human_config['data']['classes_to_relabel']:\n",
    "#     classes_to_relabel = human_config['data']['classes_to_relabel'] \n",
    "#     filtered_dataset[text_target_column] = filtered_dataset[text_target_column].replace(classes_to_relabel)\n",
    "\n",
    "# # Task specific operations - drop rows with certain classes to reduce noise \n",
    "# if len(human_config['data']['classes_to_drop']) > 0:\n",
    "#     for class_drop in human_config['data']['classes_to_drop']:\n",
    "#         filtered_dataset = filtered_dataset[filtered_dataset[text_target_column] != class_drop]\n",
    "        \n",
    "# train_data, val_data = split_and_save_dataset(filtered_dataset, text_target_column, human_config['model_training_parameters']['test_size'], human_config['model_training_parameters']['random_state'], log_dir, text_target_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/data_logs/multimodal_logs/human/train_image_human_dataset.csv\")\n",
    "val_data = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/data_logs/multimodal_logs/human/val_image_human_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image_human</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WATCH: Police helicopter flies over ferocious ...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>data_image/california_wildfires/11_10_2017/918...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDS members volunteer during California wildfi...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "      <td>data_image/california_wildfires/16_10_2017/919...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @lori_english: Parrots 22nd Floor During Ir...</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>data_image/hurricane_irma/17_9_2017/9094909252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hurricane Irma, Jose and Harvey damage: US eco...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>data_image/hurricane_harvey/8_9_2017/906112443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California Wildfire Threatening To Burn Millio...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>data_image/california_wildfires/15_10_2017/919...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  WATCH: Police helicopter flies over ferocious ...   \n",
       "1  LDS members volunteer during California wildfi...   \n",
       "2  RT @lori_english: Parrots 22nd Floor During Ir...   \n",
       "3  Hurricane Irma, Jose and Harvey damage: US eco...   \n",
       "4  California Wildfire Threatening To Burn Millio...   \n",
       "\n",
       "                              image_human  \\\n",
       "0       infrastructure_and_utility_damage   \n",
       "1  rescue_volunteering_or_donation_effort   \n",
       "2              other_relevant_information   \n",
       "3       infrastructure_and_utility_damage   \n",
       "4       infrastructure_and_utility_damage   \n",
       "\n",
       "                                          image_path  \n",
       "0  data_image/california_wildfires/11_10_2017/918...  \n",
       "1  data_image/california_wildfires/16_10_2017/919...  \n",
       "2  data_image/hurricane_irma/17_9_2017/9094909252...  \n",
       "3  data_image/hurricane_harvey/8_9_2017/906112443...  \n",
       "4  data_image/california_wildfires/15_10_2017/919...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_training_samples = select_random_n_records(train_data,25,'infrastructure_and_utility_damage',human_config['paths']['task'])\n",
    "bc_training_samples = select_random_n_records(train_data,25,'rescue_volunteering_or_donation_effort',human_config['paths']['task'])\n",
    "dc_training_samples = select_random_n_records(train_data,25,'other_relevant_information',human_config['paths']['task'])\n",
    "ef_training_samples = select_random_n_records(train_data,25,'affected_individuals',human_config['paths']['task'])\n",
    "\n",
    "training_samples_df = pd.concat([ab_training_samples,bc_training_samples,dc_training_samples,ef_training_samples],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_validation_samples = select_random_n_records(val_data,25,'infrastructure_and_utility_damage',human_config['paths']['task'])\n",
    "bc_validation_samples = select_random_n_records(val_data,25,'rescue_volunteering_or_donation_effort',human_config['paths']['task'])\n",
    "dc_validation_samples = select_random_n_records(val_data,25,'other_relevant_information',human_config['paths']['task'])\n",
    "ef_validation_samples = select_random_n_records(val_data,25,'affected_individuals',human_config['paths']['task'])\n",
    "\n",
    "validation_samples_df = pd.concat([ab_validation_samples,bc_validation_samples,dc_validation_samples,ef_validation_samples],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\"affected_individuals\":0,\n",
    "               \"infrastructure_and_utility_damage\":1,\n",
    "               \"other_relevant_information\":2,\n",
    "               \"rescue_volunteering_or_donation_effort\":3\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading for training: 100it [00:17,  5.64it/s]\n",
      "Uploading for validation: 100it [00:17,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "question_id = question_record['question_id']\n",
    "question_record = {\n",
    "    'question_id': question_id,\n",
    "    'image': None,\n",
    "    'text': None,\n",
    "    'pred':None,\n",
    "    'pred_conf':0,\n",
    "    'task': 'human',\n",
    "    'phase':None,\n",
    "    'correct_answer': None,\n",
    "    'question_options': ['Affected Individuals','Infrastructure and Utility Damage','Other Relevant Information','Rescue Volunteering or Donation Effort',  'Gather Additional Data'],\n",
    "    'question_format': 'What Type of Humanitarian Information does the Image and Text data Contain ?'\n",
    "}\n",
    "\n",
    "for index,row in tqdm(training_samples_df.iterrows(),desc=\"Uploading for training\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'train'\n",
    "    image_path = os.path.join(human_config['paths']['dataset_dir_path'],row['image_path'])\n",
    "    question_record['text'] = row['tweet_text']\n",
    "    question_record['correct_answer'] = class_names[row[f\"image_{human_config['paths']['task']}\"]]\n",
    "    upload_question(question_record,image_path)\n",
    "\n",
    "\n",
    "for index,row in tqdm(validation_samples_df.iterrows(),desc=\"Uploading for validation\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'val'\n",
    "    image_path = os.path.join(human_config['paths']['dataset_dir_path'],row['image_path'])\n",
    "    question_record['text'] = row['tweet_text']\n",
    "    question_record['correct_answer'] = class_names[row[f\"image_{human_config['paths']['task']}\"]]\n",
    "    upload_question(question_record,image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_config = load_config(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/disaster_classification/configs/damage_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, whole_dataset_csv_path = load_and_combine_tsv_data(damage_config['paths']['tsv_files'], dataset_dir=damage_config['paths']['dataset_dir_path'])\n",
    "\n",
    "# # Create folders\n",
    "# log_dir = os.path.join(main_log_dir,'multimodal_logs',damage_config['paths']['task'])\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # Filter to use only the dataset for the task columns \n",
    "# text_target_column = f\"image_{damage_config['paths']['task']}\"\n",
    "# filtered_dataset = dataset[['tweet_text', text_target_column, 'image_path']]\n",
    "# filtered_dataset = filtered_dataset.dropna(subset=[text_target_column, 'image_path'])\n",
    "\n",
    "# # Apply class relabeling if specified in the config\n",
    "# if 'classes_to_relabel' in damage_config['data'] and damage_config['data']['classes_to_relabel']:\n",
    "#     classes_to_relabel = damage_config['data']['classes_to_relabel'] \n",
    "#     filtered_dataset[text_target_column] = filtered_dataset[text_target_column].replace(classes_to_relabel)\n",
    "\n",
    "# # Task specific operations - drop rows with certain classes to reduce noise \n",
    "# if len(damage_config['data']['classes_to_drop']) > 0:\n",
    "#     for class_drop in damage_config['data']['classes_to_drop']:\n",
    "#         filtered_dataset = filtered_dataset[filtered_dataset[text_target_column] != class_drop]\n",
    "        \n",
    "# train_data, val_data = split_and_save_dataset(filtered_dataset, text_target_column, damage_config['model_training_parameters']['test_size'], damage_config['model_training_parameters']['random_state'], log_dir, text_target_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/data_logs/multimodal_logs/damage/train_image_damage_dataset.csv\")\n",
    "val_data = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/quiz_application/scripts/data_logs/multimodal_logs/damage/val_image_damage_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image_damage</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DACA decision hits tens of thousands ravaged b...</td>\n",
       "      <td>little_or_no_damage</td>\n",
       "      <td>data_image/hurricane_harvey/6_9_2017/905315238...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here's how to know if destruction caused by Hu...</td>\n",
       "      <td>little_or_no_damage</td>\n",
       "      <td>data_image/hurricane_harvey/12_9_2017/90743997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to avoid 'storm chaser' fraud after Harvey...</td>\n",
       "      <td>severe_damage</td>\n",
       "      <td>data_image/hurricane_harvey/11_9_2017/90728021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hurricane Irma, Jose and Harvey damage: US eco...</td>\n",
       "      <td>severe_damage</td>\n",
       "      <td>data_image/hurricane_harvey/8_9_2017/906112443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homes Built to Stricter Building Codes Fared B...</td>\n",
       "      <td>little_or_no_damage</td>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9098290948...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text         image_damage  \\\n",
       "0  DACA decision hits tens of thousands ravaged b...  little_or_no_damage   \n",
       "1  Here's how to know if destruction caused by Hu...  little_or_no_damage   \n",
       "2  How to avoid 'storm chaser' fraud after Harvey...        severe_damage   \n",
       "3  Hurricane Irma, Jose and Harvey damage: US eco...        severe_damage   \n",
       "4  Homes Built to Stricter Building Codes Fared B...  little_or_no_damage   \n",
       "\n",
       "                                          image_path  \n",
       "0  data_image/hurricane_harvey/6_9_2017/905315238...  \n",
       "1  data_image/hurricane_harvey/12_9_2017/90743997...  \n",
       "2  data_image/hurricane_harvey/11_9_2017/90728021...  \n",
       "3  data_image/hurricane_harvey/8_9_2017/906112443...  \n",
       "4  data_image/hurricane_irma/18_9_2017/9098290948...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_training_samples = select_random_n_records(train_data,25,'severe_damage',damage_config['paths']['task'])\n",
    "bc_training_samples = select_random_n_records(train_data,25,'little_or_no_damage',damage_config['paths']['task'])\n",
    "\n",
    "training_samples_df = pd.concat([ab_training_samples,bc_training_samples],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_validation_samples = select_random_n_records(val_data,50,'severe_damage',damage_config['paths']['task'])\n",
    "bc_validation_samples = select_random_n_records(val_data,50,'little_or_no_damage',damage_config['paths']['task'])\n",
    "\n",
    "validation_samples_df = pd.concat([ab_validation_samples,bc_validation_samples],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\"little_or_no_damage\":0,\"severe_damage\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading for training: 50it [00:08,  5.80it/s]\n",
      "Uploading for validation: 100it [00:17,  5.83it/s]\n"
     ]
    }
   ],
   "source": [
    "question_id = question_record['question_id']\n",
    "question_record = {\n",
    "    'question_id': question_id,\n",
    "    'image': None,\n",
    "    'text': None,\n",
    "    'pred':None,\n",
    "    'pred_conf':0,\n",
    "    'task': 'damage',\n",
    "    'phase':None,\n",
    "    'correct_answer': None,\n",
    "    'question_options': ['Little or No Damage', 'Severe Damage', 'Gather Additional Data'],\n",
    "    'question_format': 'Can You Identify the Level of Damage ?'\n",
    "}\n",
    "\n",
    "for index,row in tqdm(training_samples_df.iterrows(),desc=\"Uploading for training\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'train'\n",
    "    image_path = os.path.join(damage_config['paths']['dataset_dir_path'],row['image_path'])\n",
    "    question_record['text'] = row['tweet_text']\n",
    "    question_record['correct_answer'] = class_names[row[f\"image_{damage_config['paths']['task']}\"]]\n",
    "    upload_question(question_record,image_path)\n",
    "\n",
    "\n",
    "for index,row in tqdm(validation_samples_df.iterrows(),desc=\"Uploading for validation\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'val'\n",
    "    image_path = os.path.join(damage_config['paths']['dataset_dir_path'],row['image_path'])\n",
    "    question_record['text'] = row['tweet_text']\n",
    "    question_record['correct_answer'] = class_names[row[f\"image_{damage_config['paths']['task']}\"]]\n",
    "    upload_question(question_record,image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_record['question_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Data - No Damage vs Damaged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_train_df = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/post_disaster_classification/satellite_damage_vs_no_damage_saved_models/train_satellite_damage_vs_no_damage.csv\")\n",
    "sat_val_df = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/post_disaster_classification/satellite_damage_vs_no_damage_saved_models/val_satellite_damage_vs_no_damage.csv\")\n",
    "class_columns = [\"no_damage\",\"major_damage\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples_df = select_random_n_records_post_disaster(sat_train_df,100,\"major_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples_df = select_random_n_records_post_disaster(sat_val_df,100,\"major_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading for training: 100it [00:16,  6.15it/s]\n",
      "Uploading for validation: 100it [00:16,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "question_id = question_record['question_id'] # value from prev records\n",
    "question_record = {\n",
    "    'question_id': question_id,\n",
    "    'image': None,\n",
    "    'text': 'Satellite Image of the Region Impacted by the Disaster',\n",
    "    'pred':None,\n",
    "    'pred_conf':0,\n",
    "    'task': 'satellite',\n",
    "    'phase':None,\n",
    "    'correct_answer': None,\n",
    "    'question_options':['No Damage','Major Damage', 'Gather Additional Data'],\n",
    "    'question_format': 'Can You Identify the Level of Damage ?'\n",
    "}\n",
    "\n",
    "for index,row in tqdm(training_samples_df.iterrows(),desc=\"Uploading for training\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'train'\n",
    "    image_path = row['image_path']\n",
    "    if not os.path.isfile(image_path):\n",
    "        print('Does not exist : ',image_path)\n",
    "        continue\n",
    "    question_record['correct_answer'] = int(row[\"major_damage\"])\n",
    "    upload_question(question_record,image_path)\n",
    "\n",
    "\n",
    "for index,row in tqdm(validation_samples_df.iterrows(),desc=\"Uploading for validation\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'val'\n",
    "    image_path = row['image_path']\n",
    "    if not os.path.isfile(image_path):\n",
    "        print('Does not exist : ',image_path)\n",
    "        continue\n",
    "    question_record['correct_answer'] = int(row[\"major_damage\"])\n",
    "    upload_question(question_record,image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone Data : No Damage vs Damaged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "drone_train_df = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/post_disaster_classification/drone_damage_saved_models/train_drone_damage.csv\")\n",
    "drone_val_df = pd.read_csv(\"/home/julian/git-repo/juliangdz/GovernanceIRP/Autonomous-Governance-in-Disaster-Management/post_disaster_classification/drone_damage_saved_models/val_drone_damage.csv\")\n",
    "class_columns = [\"building_no_damage\",\"building_destroyed\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_destroyed_training_samples_df = select_random_n_records_post_disaster(drone_train_df,100,\"building_destroyed\")\n",
    "building_destroyed_training_samples_df = building_destroyed_training_samples_df[building_destroyed_training_samples_df[\"building_no_damage\"] != 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_destroyed_validation_samples_df = select_random_n_records_post_disaster(drone_val_df,100,\"building_destroyed\")\n",
    "building_destroyed_validation_samples_df = building_destroyed_validation_samples_df[building_destroyed_validation_samples_df[\"building_no_damage\"] != 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(building_destroyed_validation_samples_df[building_destroyed_validation_samples_df[\"building_destroyed\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(building_destroyed_validation_samples_df[building_destroyed_validation_samples_df[\"building_destroyed\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_id = question_record['question_id'] # value from prev records\n",
    "question_id # 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading for training: 95it [00:15,  5.96it/s]\n",
      "Uploading for validation: 93it [00:16,  5.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Uploading for Damage\n",
    "question_id = question_record['question_id'] # value from prev records\n",
    "question_record = {\n",
    "    'question_id': question_id,\n",
    "    'image': None,\n",
    "    'text': 'Surveillance Drone Captured Image of the Region Impacted by the Disaster',\n",
    "    'pred':None,\n",
    "    'pred_conf':0,\n",
    "    'task': 'drone-damage',\n",
    "    'phase':None,\n",
    "    'correct_answer': None,\n",
    "    'question_options':['No Damage', 'Damaged',  'Gather Additional Data'],\n",
    "    'question_format': 'Can You Identify the Level of Damage ?'\n",
    "}\n",
    "\n",
    "for index,row in tqdm(building_destroyed_training_samples_df.iterrows(),desc=\"Uploading for training\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'train'\n",
    "    image_path = row['image_path']\n",
    "    if not os.path.isfile(image_path):\n",
    "        print('Does not exist : ',image_path)\n",
    "        continue\n",
    "    question_record['correct_answer'] = int(row[\"building_destroyed\"])\n",
    "    upload_question(question_record,image_path)\n",
    "\n",
    "\n",
    "for index,row in tqdm(building_destroyed_validation_samples_df.iterrows(),desc=\"Uploading for validation\"):\n",
    "    question_record['question_id'] += 1\n",
    "    question_record['phase'] = 'val'\n",
    "    image_path = row['image_path']\n",
    "    if not os.path.isfile(image_path):\n",
    "        print('Does not exist : ',image_path)\n",
    "        continue\n",
    "    question_record['correct_answer'] = int(row['building_destroyed'])\n",
    "    upload_question(question_record,image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_no_damage_training_samples_df = select_random_n_records_post_disaster(drone_train_df,100,\"building_no_damage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_no_damage_validation_samples_df = select_random_n_records_post_disaster(drone_val_df,100,\"building_no_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_id = question_record['question_id'] # value from prev records\n",
    "# question_id # 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uploading for No Damage\n",
    "# question_id = question_record['question_id'] # value from prev records\n",
    "# question_record = {\n",
    "#     'question_id': question_id,\n",
    "#     'image': None,\n",
    "#     'text': 'Surveillance Drone Captured Image of the Region Impacted by the Disaster',\n",
    "#     'pred':None,\n",
    "#     'pred_conf':0,\n",
    "#     'task': 'drone-no_damage',\n",
    "#     'phase':None,\n",
    "#     'correct_answer': None,\n",
    "#     'question_options': ['Actionable Data', 'Gather Additional Data', 'No Response Necessary'],\n",
    "#     'question_format': 'Can You Identify the Level of Damage ?'\n",
    "# }\n",
    "\n",
    "# for index,row in tqdm(building_no_damage_training_samples_df.iterrows(),desc=\"Uploading for training\"):\n",
    "#     question_record['question_id'] += 1\n",
    "#     question_record['phase'] = 'train'\n",
    "#     image_path = row['image_path']\n",
    "#     if not os.path.isfile(image_path):\n",
    "#         print('Does not exist : ',image_path)\n",
    "#         continue\n",
    "#     question_record['correct_answer'] = 0\n",
    "#     upload_question(question_record,image_path)\n",
    "\n",
    "\n",
    "# for index,row in tqdm(building_no_damage_validation_samples_df.iterrows(),desc=\"Uploading for validation\"):\n",
    "#     question_record['question_id'] += 1\n",
    "#     question_record['phase'] = 'val'\n",
    "#     image_path = row['image_path']\n",
    "#     if not os.path.isfile(image_path):\n",
    "#         print('Does not exist : ',image_path)\n",
    "#         continue\n",
    "#     question_record['correct_answer'] = 0\n",
    "#     upload_question(question_record,image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_id = question_record['question_id'] # value from prev records\n",
    "# question_id # 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
